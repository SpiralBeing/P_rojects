{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tokenization\n",
    "#The file \"tokenization\" is forked from:\n",
    "#https://github.com/google-research/bert/blob/master/tokenization.py.\n",
    "\n",
    "#https://www.kaggle.com/wrrosa/keras-bert-using-tfhub-modified-train-data/data?select=tokenization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions  \n",
    "Source for bert_encode function: https://www.kaggle.com/user123454321/bert-starter-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    print(sequence_output)\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    print(clf_output.shape)\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.39 s, sys: 1.05 s, total: 9.45 s\n",
      "Wall time: 9.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = pd.read_csv(\"nlp-getting-started/train.csv\")\n",
    "#test = pd.read_csv(\"nlp-getting-started/test.csv\")\n",
    "\n",
    "train = df_train\n",
    "#test = X_train.text\n",
    "max_len = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = bert_encode(train.text.values, tokenizer, max_len=max_len)\n",
    "#test_input = bert_encode(test.text.values, tokenizer, max_len=max_len)\n",
    "train_labels = train.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (6989, 90)\n"
     ]
    }
   ],
   "source": [
    "print(len(train_input), train_input[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = train_input[0]\n",
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr = LogisticRegression()\n",
    "clf_lr.fit(train_input, train_labels)\n",
    "\n",
    "predictions = clf_lr.predict(train_input)\n",
    "print(metrics.confusion_matrix(train_labels, predictions))\n",
    "# Print a classification report\n",
    "print(metrics.classification_report(train_labels, predictions))\n",
    "# Print the overall accuracy\n",
    "print(metrics.accuracy_score(train_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"keras_layer/StatefulPartitionedCall_1:1\", shape=(None, None, 1024), dtype=float32)\n",
      "(None, 1024)\n",
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 90)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 90)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 90)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 1024)]       0           keras_layer[1][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            1025        tf_op_layer_strided_slice_1[0][0]\n",
      "==================================================================================================\n",
      "Total params: 335,142,914\n",
      "Trainable params: 1,025\n",
      "Non-trainable params: 335,141,889\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_layer, max_len=max_len)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "70/70 [==============================] - 304s 4s/step - loss: 0.5945 - accuracy: 0.6961 - val_loss: 0.5314 - val_accuracy: 0.7482\n",
      "Epoch 2/4\n",
      "70/70 [==============================] - 302s 4s/step - loss: 0.5154 - accuracy: 0.7541 - val_loss: 0.4946 - val_accuracy: 0.7747\n",
      "Epoch 3/4\n",
      "70/70 [==============================] - 302s 4s/step - loss: 0.4919 - accuracy: 0.7716 - val_loss: 0.4750 - val_accuracy: 0.7876\n",
      "Epoch 4/4\n",
      "70/70 [==============================] - 300s 4s/step - loss: 0.4762 - accuracy: 0.7845 - val_loss: 0.4680 - val_accuracy: 0.7876\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "train_history = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=4,\n",
    "    callbacks=[checkpoint],\n",
    "    batch_size=80,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict test dataset to submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = bert_encode(test.text.values, tokenizer, max_len=max_len)\n",
    "test_pred = model.predict(test_input)\n",
    "submission = train.truncate(after = -1)\n",
    "submission['id'] = test['id']\n",
    "submission['text'] = test['text']\n",
    "submission['target'] = test_pred.round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission.drop(['text'], axis=1, inplace=True)\n",
    "submission = submission[['id','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"nlp-getting-started/answer.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of BERT \n",
    "_______________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"nlp-getting-started/train.csv\")\n",
    "df_test = pd.read_csv(\"nlp-getting-started/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5) (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "display(df_train.head())\n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[0,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop([\"keyword\",\"location\"], axis=1, inplace=True)\n",
    "df_test.drop([\"keyword\",\"location\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  target\n",
       "0   1  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1   4             Forest fire near La Ronge Sask. Canada       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "text      0\n",
       "target    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train.target.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Не сильная разбалансированность. Оставим все как есть</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pandas(Index=0, id=1, text='Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all', target=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(df_train.itertuples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect & remove empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 blanks:  []\n"
     ]
    }
   ],
   "source": [
    "blanks = []\n",
    "\n",
    "for index, i, text, target in df_train.itertuples():  # iterate over the DataFrame\n",
    "    if type(text)==str:            # avoid NaN values\n",
    "        if text.isspace():         # test 'review' for whitespace\n",
    "            blanks.append(i) \n",
    "        \n",
    "print(len(blanks), 'blanks: ', blanks)\n",
    "\n",
    "df_train.drop(blanks, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Избавимся от ссылок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#stormchase Violent Record Breaking EF-5 El Reno Oklahoma Tornado Nearly Runs Over ... - http://t.co/3SICroAaNz http://t.co/I27Oa0HISp\n",
      "['http://t.co/3SICroAaNz', 'http://t.co/I27Oa0HISp']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#stormchase Violent Record Breaking EF-5 El Reno Oklahoma Tornado Nearly Runs Over ... -  '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = df_train[\"text\"].head(-5).values[-1]\n",
    "print(line)\n",
    "\n",
    "pattern = r'http://[/.\\w]+'#cuz maybe https://... or http://  or just http\n",
    "print(re.findall(pattern, line))\n",
    "\n",
    "re.sub(pattern,'',line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rid_of_link(text):\n",
    "    raw_s = r'{}'.format(text)\n",
    "    pattern = r'http[:/.\\w]+'\n",
    "    raw_s = re.sub(pattern,'',raw_s)\n",
    "    return(raw_s)\n",
    "\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(get_rid_of_link)\n",
    "df_test[\"text\"] = df_test[\"text\"].apply(get_rid_of_link)\n",
    "df_train.to_csv(\"nlp-getting-started/train_without_link.csv\", index=False)\n",
    "df_test.to_csv(\"nlp-getting-started/test_without_link.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Find time (am/pm/UTC/..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10:34:24', 'UTC', '06:34:24', '4:00']\n",
      "Earthquake : M 3.4 - 96km N of Brenas Puerto Rico: Time2015-08-05  2015-08-05  - atÛ_\n",
      "Earthquake : M 3.4 - 96km N of Brenas Puerto Rico: Time2015-08-05 10:34:24 UTC2015-08-05 06:34:24 -4:00 atÛ_\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"[\\d]+:[\\d]+:[\\d]+\"\n",
    "pattern_2 = r\"[\\d]+:[\\d]+\"\n",
    "pattern_3 = r\"(am|pm|UTC)\"\n",
    "\n",
    "pattern = r\"[\\d]+:[\\d]+:[\\d]+|[\\d]+:[\\d]+|am|pm|UTC\"\n",
    "\n",
    "line = r\"Earthquake : M 3.4 - 96km N of Brenas Puerto Rico: Time2015-08-05 10:34:24 UTC2015-08-05 06:34:24 -4:00 atÛ_\"\n",
    "print(re.findall(pattern, line))\n",
    "print(re.sub(pattern, \"\",line))\n",
    "print(line)\n",
    "\n",
    "line = r\"Meow, Sparta\"\n",
    "print(re.findall(pattern, line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bool_time_feature(text):\n",
    "    raw_s = r'{}'.format(text)\n",
    "    pattern = r\"[\\d]+:[\\d]+:[\\d]+|[\\d]+:[\\d]+|am|pm|UTC\"\n",
    "    if(len(re.findall(pattern, raw_s))!=0):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "    \n",
    "def get_rid_of_time(text):\n",
    "    raw_s = r'{}'.format(text)\n",
    "    pattern = r\"[\\d]+:[\\d]+:[\\d]+|[\\d]+:[\\d]+|am|pm|UTC\"\n",
    "    raw_s = re.sub(pattern,'',raw_s)\n",
    "    return(raw_s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"time\"] = df_train[\"text\"].apply(create_bool_time_feature)\n",
    "df_test[\"time\"] = df_test[\"text\"].apply(create_bool_time_feature)\n",
    "\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(get_rid_of_time)\n",
    "df_test[\"text\"] = df_test[\"text\"].apply(get_rid_of_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5898\n",
       "1    1715\n",
       "Name: time, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"time\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Удалим дубликаты \n",
    "Есть дубликаты. Некоторые наблюдения полностью совпадают по \"text\", некоторые отличаются орфографической ошибкой в тексте.  \n",
    "Удалим те, что полностью идентичны по feature \"text\" (значения \"taget\" порой разные)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of observations: (7613,),\n",
      "Number of unique observations: (6989,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Amount of observations: {df_train.text.shape},\\nNumber of unique observations: {df_train.text.unique().shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Find Countries, cities, states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport spacy\\nnlp = spacy.load('en_core_web_md')#small version\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')#small version\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef create_bool_GPE(text):\\n    raw_s = nlp(u\\'{}\\'.format(text))\\n    GPE_list = [1 for val in raw_s.ents if val.label_==\"GPE\"]\\n    if(len(GPE_list)!=0):\\n        return(1)\\n    else:\\n        return(0)\\n    \\ndef get_rid_of_GPE(text):\\n    raw_s = r\\'{}\\'.format(text)\\n    raw_s_nlp = nlp(u\\'{}\\'.format(text))\\n    GPE_list = [val.text for val in raw_s_nlp.ents if val.label_==\"GPE\"]\\n    for GPE_item in GPE_list:\\n        try:\\n            raw_s = re.sub(GPE_item, \\'\\', raw_s)\\n        except:\\n            pass\\n    return(raw_s)\\n\\n\\n\\ndoc8 = nlp(u\\'A Cessna airplane accident in Ocampo Coahuila Mexico on July 29 2015 killed four men including a State of Coahuila government official. Horrible Accident Man Died In Wings of Airplane (29-07-2015)\\')\\n\\nGPE_list = [val.text for val in doc8.ents if val.label_==\"GPE\"]\\nprint(GPE_list)\\nprint(doc8)\\nprint(get_rid_of_GPE(doc8))\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def create_bool_GPE(text):\n",
    "    raw_s = nlp(u'{}'.format(text))\n",
    "    GPE_list = [1 for val in raw_s.ents if val.label_==\"GPE\"]\n",
    "    if(len(GPE_list)!=0):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "    \n",
    "def get_rid_of_GPE(text):\n",
    "    raw_s = r'{}'.format(text)\n",
    "    raw_s_nlp = nlp(u'{}'.format(text))\n",
    "    GPE_list = [val.text for val in raw_s_nlp.ents if val.label_==\"GPE\"]\n",
    "    for GPE_item in GPE_list:\n",
    "        try:\n",
    "            raw_s = re.sub(GPE_item, '', raw_s)\n",
    "        except:\n",
    "            pass\n",
    "    return(raw_s)\n",
    "\n",
    "\n",
    "\n",
    "doc8 = nlp(u'A Cessna airplane accident in Ocampo Coahuila Mexico on July 29 2015 killed four men including a State of Coahuila government official. Horrible Accident Man Died In Wings of Airplane (29-07-2015)')\n",
    "\n",
    "GPE_list = [val.text for val in doc8.ents if val.label_==\"GPE\"]\n",
    "print(GPE_list)\n",
    "print(doc8)\n",
    "print(get_rid_of_GPE(doc8))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf_train[\"GPE\"] = df_train[\"text\"].apply(create_bool_GPE)\\ndf_test[\"GPE\"] = df_test[\"text\"].apply(create_bool_GPE)\\n\\ndf_train[\"text\"] = df_train[\"text\"].apply(get_rid_of_GPE)\\ndf_test[\"text\"] = df_test[\"text\"].apply(get_rid_of_GPE)\\n\\ndf_train[\"GPE\"].value_counts()\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df_train[\"GPE\"] = df_train[\"text\"].apply(create_bool_GPE)\n",
    "df_test[\"GPE\"] = df_test[\"text\"].apply(create_bool_GPE)\n",
    "\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(get_rid_of_GPE)\n",
    "df_test[\"text\"] = df_test[\"text\"].apply(get_rid_of_GPE)\n",
    "\n",
    "df_train[\"GPE\"].value_counts()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf_train.to_csv(\"nlp-getting-started/df_train_cleaned.csv\", index=False)\\ndf_test.to_csv(\"nlp-getting-started/df_test_cleaned.csv\", index=False)\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df_train.to_csv(\"nlp-getting-started/df_train_cleaned.csv\", index=False)\n",
    "df_test.to_csv(\"nlp-getting-started/df_test_cleaned.csv\", index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  Удаление всех токенов вида цифры/цифры+слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rid_of_digits(text):\n",
    "    raw_s = r'{}'.format(text)\n",
    "    pattern = r\"\\d+\\w+|\\w+\\d+\"\n",
    "    raw_s = re.sub(pattern,'',raw_s)\n",
    "    return(raw_s)\n",
    "\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(get_rid_of_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Удаление Тэгов (`#`... и @....) и слов с подчеркиванием (_ashj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rid_of_tags(text):\n",
    "    raw_s = r'{}'.format(text)\n",
    "    pattern = r\"@\\w+|#\\w+|_+\\w+|\\w+_|\"\n",
    "    raw_s = re.sub(pattern,'',raw_s)\n",
    "    return(raw_s)\n",
    "\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(get_rid_of_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Отбор слов (создание списка stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aa': 0,\n",
       " 'aaaa': 1,\n",
       " 'aaaaaaallll': 2,\n",
       " 'aaaaaand': 3,\n",
       " 'aaarrrgghhh': 4,\n",
       " 'aan': 5,\n",
       " 'aannnnd': 6,\n",
       " 'aar': 7,\n",
       " 'aashiqui': 8,\n",
       " 'ab': 9,\n",
       " 'aba': 10,\n",
       " 'abandon': 11,\n",
       " 'abandoned': 12,\n",
       " 'abandoning': 13,\n",
       " 'abbandoned': 14,\n",
       " 'abbott': 15,\n",
       " 'abbruchsimulator': 16,\n",
       " 'abbswinston': 17,\n",
       " 'abc': 18,\n",
       " 'abcnews': 19,\n",
       " 'abe': 20,\n",
       " 'aberdeen': 21,\n",
       " 'aberystwyth': 22,\n",
       " 'abia': 23,\n",
       " 'ability': 24,\n",
       " 'abject': 25,\n",
       " 'ablaze': 26,\n",
       " 'able': 27,\n",
       " 'aboard': 28,\n",
       " 'abomination': 29,\n",
       " 'abortion': 30,\n",
       " 'abortions': 31,\n",
       " 'abouts': 32,\n",
       " 'abq': 33,\n",
       " 'abs': 34,\n",
       " 'absence': 35,\n",
       " 'absolute': 36,\n",
       " 'absolutely': 37,\n",
       " 'abstract': 38,\n",
       " 'absurd': 39,\n",
       " 'absurdly': 40,\n",
       " 'abuse': 41,\n",
       " 'abused': 42,\n",
       " 'abuses': 43,\n",
       " 'abusing': 44,\n",
       " 'ac': 45,\n",
       " 'academia': 46,\n",
       " 'acc': 47,\n",
       " 'accept': 48,\n",
       " 'accepte': 49,\n",
       " 'accepts': 50,\n",
       " 'access': 51,\n",
       " 'accident': 52,\n",
       " 'accidentally': 53,\n",
       " 'accidently': 54,\n",
       " 'accidents': 55,\n",
       " 'accompanying': 56,\n",
       " 'according': 57,\n",
       " 'accordingly': 58,\n",
       " 'account': 59,\n",
       " 'accountable': 60,\n",
       " 'accounts': 61,\n",
       " 'accuracy': 62,\n",
       " 'accused': 63,\n",
       " 'accuses': 64,\n",
       " 'accustomed': 65,\n",
       " 'acdelco': 66,\n",
       " 'ace': 67,\n",
       " 'acesse': 68,\n",
       " 'achieve': 69,\n",
       " 'achievement': 70,\n",
       " 'achieving': 71,\n",
       " 'achimota': 72,\n",
       " 'aching': 73,\n",
       " 'acid': 74,\n",
       " 'acids': 75,\n",
       " 'acne': 76,\n",
       " 'acoustic': 77,\n",
       " 'acquiesce': 78,\n",
       " 'acquire': 79,\n",
       " 'acquired': 80,\n",
       " 'acquisitions': 81,\n",
       " 'acres': 82,\n",
       " 'acronym': 83,\n",
       " 'acrylic': 84,\n",
       " 'act': 85,\n",
       " 'actavis': 86,\n",
       " 'acted': 87,\n",
       " 'actin': 88,\n",
       " 'acting': 89,\n",
       " 'action': 90,\n",
       " 'actions': 91,\n",
       " 'activate': 92,\n",
       " 'activated': 93,\n",
       " 'activates': 94,\n",
       " 'active': 95,\n",
       " 'actively': 96,\n",
       " 'activision': 97,\n",
       " 'activist': 98,\n",
       " 'activities': 99,\n",
       " 'activity': 100,\n",
       " 'actor': 101,\n",
       " 'actress': 102,\n",
       " 'acts': 103,\n",
       " 'actual': 104,\n",
       " 'actually': 105,\n",
       " 'acura': 106,\n",
       " 'acute': 107,\n",
       " 'ad': 108,\n",
       " 'adantly': 109,\n",
       " 'adaptation': 110,\n",
       " 'add': 111,\n",
       " 'added': 112,\n",
       " 'addict': 113,\n",
       " 'addiction': 114,\n",
       " 'addicts': 115,\n",
       " 'adding': 116,\n",
       " 'addition': 117,\n",
       " 'address': 118,\n",
       " 'addresses': 119,\n",
       " 'adelaide': 120,\n",
       " 'adhd': 121,\n",
       " 'adidas': 122,\n",
       " 'adjustable': 123,\n",
       " 'adjusted': 124,\n",
       " 'adjuster': 125,\n",
       " 'admin': 126,\n",
       " 'administration': 127,\n",
       " 'administrative': 128,\n",
       " 'admit': 129,\n",
       " 'admits': 130,\n",
       " 'adopt': 131,\n",
       " 'adoption': 132,\n",
       " 'adoptive': 133,\n",
       " 'adorable': 134,\n",
       " 'ads': 135,\n",
       " 'adult': 136,\n",
       " 'adults': 137,\n",
       " 'advance': 138,\n",
       " 'advanced': 139,\n",
       " 'advances': 140,\n",
       " 'advantages': 141,\n",
       " 'adventures': 142,\n",
       " 'adverse': 143,\n",
       " 'advertise': 144,\n",
       " 'advertised': 145,\n",
       " 'advice': 146,\n",
       " 'advised': 147,\n",
       " 'advisory': 148,\n",
       " 'aeg': 149,\n",
       " 'aesop': 150,\n",
       " 'aesthetic': 151,\n",
       " 'af': 152,\n",
       " 'affair': 153,\n",
       " 'affect': 154,\n",
       " 'affected': 155,\n",
       " 'affecting': 156,\n",
       " 'affects': 157,\n",
       " 'affiliate': 158,\n",
       " 'affiliation': 159,\n",
       " 'affleck': 160,\n",
       " 'affliction': 161,\n",
       " 'afghan': 162,\n",
       " 'afghanistan': 163,\n",
       " 'afghetc': 164,\n",
       " 'afloat': 165,\n",
       " 'afp': 166,\n",
       " 'afraid': 167,\n",
       " 'africa': 168,\n",
       " 'african': 169,\n",
       " 'africans': 170,\n",
       " 'africaå': 171,\n",
       " 'afrikaan': 172,\n",
       " 'afrin': 173,\n",
       " 'afte': 174,\n",
       " 'aftermath': 175,\n",
       " 'afternoon': 176,\n",
       " 'aftershock': 177,\n",
       " 'aftershocks': 178,\n",
       " 'afycso': 179,\n",
       " 'ag': 180,\n",
       " 'agalloch': 181,\n",
       " 'agdq': 182,\n",
       " 'age': 183,\n",
       " 'aged': 184,\n",
       " 'agencies': 185,\n",
       " 'agency': 186,\n",
       " 'agent': 187,\n",
       " 'agents': 188,\n",
       " 'ages': 189,\n",
       " 'aggarwal': 190,\n",
       " 'aggressif': 191,\n",
       " 'aggression': 192,\n",
       " 'aggressive': 193,\n",
       " 'aggressively': 194,\n",
       " 'agnivesh': 195,\n",
       " 'agnus': 196,\n",
       " 'ago': 197,\n",
       " 'agony': 198,\n",
       " 'agree': 199,\n",
       " 'agreed': 200,\n",
       " 'agreements': 201,\n",
       " 'agrees': 202,\n",
       " 'aguero': 203,\n",
       " 'agw': 204,\n",
       " 'ah': 205,\n",
       " 'ahahahga': 206,\n",
       " 'ahead': 207,\n",
       " 'ahedis': 208,\n",
       " 'ahh': 209,\n",
       " 'ahhh': 210,\n",
       " 'ahhhh': 211,\n",
       " 'ahhhhh': 212,\n",
       " 'ahrar': 213,\n",
       " 'ai': 214,\n",
       " 'aia': 215,\n",
       " 'aid': 216,\n",
       " 'aidan': 217,\n",
       " 'aiii': 218,\n",
       " 'aim': 219,\n",
       " 'aimlessly': 220,\n",
       " 'ain': 221,\n",
       " 'air': 222,\n",
       " 'airasia': 223,\n",
       " 'aircraft': 224,\n",
       " 'airhead': 225,\n",
       " 'airhorns': 226,\n",
       " 'airing': 227,\n",
       " 'airlift': 228,\n",
       " 'airlines': 229,\n",
       " 'airplane': 230,\n",
       " 'airplaneåê': 231,\n",
       " 'airport': 232,\n",
       " 'airports': 233,\n",
       " 'airstrikes': 234,\n",
       " 'aisle': 235,\n",
       " 'ajw': 236,\n",
       " 'ak': 237,\n",
       " 'aka': 238,\n",
       " 'ake': 239,\n",
       " 'akilah': 240,\n",
       " 'akito': 241,\n",
       " 'akr': 242,\n",
       " 'aks': 243,\n",
       " 'akwa': 244,\n",
       " 'akx': 245,\n",
       " 'akxbskdn': 246,\n",
       " 'al': 247,\n",
       " 'alaba': 248,\n",
       " 'alabaquake': 249,\n",
       " 'aladdin': 250,\n",
       " 'alarm': 251,\n",
       " 'alarmed': 252,\n",
       " 'alarming': 253,\n",
       " 'alarmingly': 254,\n",
       " 'alarms': 255,\n",
       " 'alas': 256,\n",
       " 'alaska': 257,\n",
       " 'alaskan': 258,\n",
       " 'alba': 259,\n",
       " 'albany': 260,\n",
       " 'albeit': 261,\n",
       " 'alberta': 262,\n",
       " 'albertans': 263,\n",
       " 'albertsons': 264,\n",
       " 'album': 265,\n",
       " 'albums': 266,\n",
       " 'alchemist': 267,\n",
       " 'alcohol': 268,\n",
       " 'alcoholism': 269,\n",
       " 'aldridge': 270,\n",
       " 'alec': 271,\n",
       " 'alert': 272,\n",
       " 'alerts': 273,\n",
       " 'alex': 274,\n",
       " 'alexandrian': 275,\n",
       " 'alexis': 276,\n",
       " 'algae': 277,\n",
       " 'alhaji': 278,\n",
       " 'ali': 279,\n",
       " 'alice': 280,\n",
       " 'alien': 281,\n",
       " 'aliens': 282,\n",
       " 'align': 283,\n",
       " 'alil': 284,\n",
       " 'alisonannyoung': 285,\n",
       " 'alive': 286,\n",
       " 'allah': 287,\n",
       " 'allay': 288,\n",
       " 'allegations': 289,\n",
       " 'alleged': 290,\n",
       " 'allegedly': 291,\n",
       " 'allegiance': 292,\n",
       " 'allergic': 293,\n",
       " 'alley': 294,\n",
       " 'alliance': 295,\n",
       " 'allied': 296,\n",
       " 'allies': 297,\n",
       " 'alllll': 298,\n",
       " 'allocating': 299,\n",
       " 'alloosh': 300,\n",
       " 'allotment': 301,\n",
       " 'allow': 302,\n",
       " 'allowed': 303,\n",
       " 'allowing': 304,\n",
       " 'allows': 305,\n",
       " 'alloy': 306,\n",
       " 'ally': 307,\n",
       " 'almighty': 308,\n",
       " 'alois': 309,\n",
       " 'alot': 310,\n",
       " 'alpha': 311,\n",
       " 'alphen': 312,\n",
       " 'alps': 313,\n",
       " 'alright': 314,\n",
       " 'alrighty': 315,\n",
       " 'alska': 316,\n",
       " 'alsowhat': 317,\n",
       " 'alternate': 318,\n",
       " 'alternative': 319,\n",
       " 'alternatives': 320,\n",
       " 'alton': 321,\n",
       " 'altonte': 322,\n",
       " 'aluminum': 323,\n",
       " 'alves': 324,\n",
       " 'alwsl': 325,\n",
       " 'ama': 326,\n",
       " 'amalie': 327,\n",
       " 'amateur': 328,\n",
       " 'amazing': 329,\n",
       " 'amazon': 330,\n",
       " 'amazondeals': 331,\n",
       " 'amber': 332,\n",
       " 'ambleside': 333,\n",
       " 'ambulance': 334,\n",
       " 'amcx': 335,\n",
       " 'amen': 336,\n",
       " 'ameribag': 337,\n",
       " 'america': 338,\n",
       " 'american': 339,\n",
       " 'americans': 340,\n",
       " 'ames': 341,\n",
       " 'amico': 342,\n",
       " 'amid': 343,\n",
       " 'amiibos': 344,\n",
       " 'amirite': 345,\n",
       " 'amman': 346,\n",
       " 'amplifier': 347,\n",
       " 'amritsar': 348,\n",
       " 'amsal': 349,\n",
       " 'amsterd': 350,\n",
       " 'ana': 351,\n",
       " 'anakin': 352,\n",
       " 'analog': 353,\n",
       " 'analysis': 354,\n",
       " 'anatomy': 355,\n",
       " 'anchor': 356,\n",
       " 'anchorage': 357,\n",
       " 'anchors': 358,\n",
       " 'ancient': 359,\n",
       " 'ancop': 360,\n",
       " 'anders': 361,\n",
       " 'anderson': 362,\n",
       " 'andre': 363,\n",
       " 'andrea': 364,\n",
       " 'andrew': 365,\n",
       " 'ands': 366,\n",
       " 'andy': 367,\n",
       " 'andåêchina': 368,\n",
       " 'anew': 369,\n",
       " 'angel': 370,\n",
       " 'angela': 371,\n",
       " 'angeles': 372,\n",
       " 'angelina': 373,\n",
       " 'angelriveralib': 374,\n",
       " 'angels': 375,\n",
       " 'anger': 376,\n",
       " 'angers': 377,\n",
       " 'angioplasty': 378,\n",
       " 'angry': 379,\n",
       " 'ani': 380,\n",
       " 'animal': 381,\n",
       " 'animals': 382,\n",
       " 'animations': 383,\n",
       " 'animatronics': 384,\n",
       " 'anime': 385,\n",
       " 'aniston': 386,\n",
       " 'anjem': 387,\n",
       " 'ankle': 388,\n",
       " 'ankles': 389,\n",
       " 'anna': 390,\n",
       " 'annddd': 391,\n",
       " 'annihilate': 392,\n",
       " 'annihilated': 393,\n",
       " 'annihilating': 394,\n",
       " 'annihilation': 395,\n",
       " 'anniversary': 396,\n",
       " 'announce': 397,\n",
       " 'announced': 398,\n",
       " 'announcement': 399,\n",
       " 'announces': 400,\n",
       " 'annoyed': 401,\n",
       " 'annoying': 402,\n",
       " 'annual': 403,\n",
       " 'anonymous': 404,\n",
       " 'answer': 405,\n",
       " 'answered': 406,\n",
       " 'answers': 407,\n",
       " 'ant': 408,\n",
       " 'ante': 409,\n",
       " 'anthelmintic': 410,\n",
       " 'anthology': 411,\n",
       " 'anthony': 412,\n",
       " 'anthonys': 413,\n",
       " 'anthrax': 414,\n",
       " 'anti': 415,\n",
       " 'antichrist': 416,\n",
       " 'antioch': 417,\n",
       " 'antiochhickoryhollow': 418,\n",
       " 'antiochus': 419,\n",
       " 'antipozi': 420,\n",
       " 'antonio': 421,\n",
       " 'antony': 422,\n",
       " 'ants': 423,\n",
       " 'anu': 424,\n",
       " 'anxiety': 425,\n",
       " 'anxious': 426,\n",
       " 'anybody': 427,\n",
       " 'anymore': 428,\n",
       " 'anytime': 429,\n",
       " 'anyways': 430,\n",
       " 'anza': 431,\n",
       " 'aogashima': 432,\n",
       " 'ap': 433,\n",
       " 'apart': 434,\n",
       " 'apartment': 435,\n",
       " 'apartments': 436,\n",
       " 'apaz': 437,\n",
       " 'apc': 438,\n",
       " 'apch': 439,\n",
       " 'apcåêpdp': 440,\n",
       " 'apd': 441,\n",
       " 'apga': 442,\n",
       " 'apiece': 443,\n",
       " 'apocalpytic': 444,\n",
       " 'apocalypse': 445,\n",
       " 'apocalyptic': 446,\n",
       " 'apollo': 447,\n",
       " 'apologies': 448,\n",
       " 'apologise': 449,\n",
       " 'apologize': 450,\n",
       " 'apologized': 451,\n",
       " 'app': 452,\n",
       " 'appalling': 453,\n",
       " 'apparent': 454,\n",
       " 'apparently': 455,\n",
       " 'appeals': 456,\n",
       " 'appeared': 457,\n",
       " 'appears': 458,\n",
       " 'appease': 459,\n",
       " 'apperception': 460,\n",
       " 'appetite': 461,\n",
       " 'applaud': 462,\n",
       " 'apple': 463,\n",
       " 'applications': 464,\n",
       " 'applied': 465,\n",
       " 'applies': 466,\n",
       " 'apply': 467,\n",
       " 'appointment': 468,\n",
       " 'appoints': 469,\n",
       " 'appraisal': 470,\n",
       " 'appreciate': 471,\n",
       " 'appreciated': 472,\n",
       " 'approach': 473,\n",
       " 'approaches': 474,\n",
       " 'approaching': 475,\n",
       " 'appropriate': 476,\n",
       " 'appropriation': 477,\n",
       " 'approval': 478,\n",
       " 'approves': 479,\n",
       " 'appx': 480,\n",
       " 'apr': 481,\n",
       " 'april': 482,\n",
       " 'apropos': 483,\n",
       " 'apt': 484,\n",
       " 'apts': 485,\n",
       " 'aq': 486,\n",
       " 'aqua': 487,\n",
       " 'aquarium': 488,\n",
       " 'aquarius': 489,\n",
       " 'ar': 490,\n",
       " 'ara': 491,\n",
       " 'arab': 492,\n",
       " 'arabia': 493,\n",
       " 'arabian': 494,\n",
       " 'arabic': 495,\n",
       " 'arachys': 496,\n",
       " 'arcade': 497,\n",
       " 'arceen': 498,\n",
       " 'archetype': 499,\n",
       " 'architect': 500,\n",
       " 'architects': 501,\n",
       " 'architecture': 502,\n",
       " 'area': 503,\n",
       " 'areal': 504,\n",
       " 'areas': 505,\n",
       " 'aren': 506,\n",
       " 'arena': 507,\n",
       " 'arent': 508,\n",
       " 'areva': 509,\n",
       " 'arfur': 510,\n",
       " 'argentina': 511,\n",
       " 'argentinean': 512,\n",
       " 'argh': 513,\n",
       " 'argsuppose': 514,\n",
       " 'argue': 515,\n",
       " 'argues': 516,\n",
       " 'argument': 517,\n",
       " 'ari': 518,\n",
       " 'arian': 519,\n",
       " 'ariana': 520,\n",
       " 'arin': 521,\n",
       " 'arizona': 522,\n",
       " 'arkan': 523,\n",
       " 'arlington': 524,\n",
       " 'arm': 525,\n",
       " 'armageddon': 526,\n",
       " 'armed': 527,\n",
       " 'armenians': 528,\n",
       " 'armory': 529,\n",
       " 'arms': 530,\n",
       " 'army': 531,\n",
       " 'arnhem': 532,\n",
       " 'arnley': 533,\n",
       " 'arranged': 534,\n",
       " 'arreat': 535,\n",
       " 'arrest': 536,\n",
       " 'arrested': 537,\n",
       " 'arrests': 538,\n",
       " 'arrival': 539,\n",
       " 'arrive': 540,\n",
       " 'arrived': 541,\n",
       " 'arrives': 542,\n",
       " 'arriving': 543,\n",
       " 'arrogant': 544,\n",
       " 'ars': 545,\n",
       " 'arse': 546,\n",
       " 'arsenal': 547,\n",
       " 'arson': 548,\n",
       " 'arsonist': 549,\n",
       " 'arsonists': 550,\n",
       " 'art': 551,\n",
       " 'arti': 552,\n",
       " 'articals': 553,\n",
       " 'article': 554,\n",
       " 'articles': 555,\n",
       " 'artificial': 556,\n",
       " 'artillery': 557,\n",
       " 'artist': 558,\n",
       " 'artists': 559,\n",
       " 'arts': 560,\n",
       " 'artwork': 561,\n",
       " 'ary': 562,\n",
       " 'asap': 563,\n",
       " 'asb': 564,\n",
       " 'asbury': 565,\n",
       " 'ascend': 566,\n",
       " 'aseer': 567,\n",
       " 'asf': 568,\n",
       " 'ash': 569,\n",
       " 'ashayo': 570,\n",
       " 'ashdod': 571,\n",
       " 'ashenforest': 572,\n",
       " 'ashes': 573,\n",
       " 'ashley': 574,\n",
       " 'ashrafiyah': 575,\n",
       " 'asia': 576,\n",
       " 'asian': 577,\n",
       " 'asics': 578,\n",
       " 'aside': 579,\n",
       " 'ask': 580,\n",
       " 'asked': 581,\n",
       " 'askin': 582,\n",
       " 'asking': 583,\n",
       " 'asks': 584,\n",
       " 'asleep': 585,\n",
       " 'aspect': 586,\n",
       " 'aspects': 587,\n",
       " 'asphalt': 588,\n",
       " 'aspiring': 589,\n",
       " 'ass': 590,\n",
       " 'assad': 591,\n",
       " 'assailant': 592,\n",
       " 'assault': 593,\n",
       " 'assembly': 594,\n",
       " 'assertative': 595,\n",
       " 'asses': 596,\n",
       " 'assessment': 597,\n",
       " 'assets': 598,\n",
       " 'asshole': 599,\n",
       " 'assholes': 600,\n",
       " 'assistance': 601,\n",
       " 'assistant': 602,\n",
       " 'assisting': 603,\n",
       " 'associated': 604,\n",
       " 'association': 605,\n",
       " 'assume': 606,\n",
       " 'assumes': 607,\n",
       " 'assured': 608,\n",
       " 'asswipe': 609,\n",
       " 'astonishing': 610,\n",
       " 'astounding': 611,\n",
       " 'astrakhan': 612,\n",
       " 'astrologian': 613,\n",
       " 'astroturfers': 614,\n",
       " 'asylum': 615,\n",
       " 'atathon': 616,\n",
       " 'atc': 617,\n",
       " 'atcha': 618,\n",
       " 'ate': 619,\n",
       " 'athens': 620,\n",
       " 'athlete': 621,\n",
       " 'athletics': 622,\n",
       " 'atl': 623,\n",
       " 'atlanta': 624,\n",
       " 'atlantic': 625,\n",
       " 'atlas': 626,\n",
       " 'atleast': 627,\n",
       " 'atm': 628,\n",
       " 'atmosphere': 629,\n",
       " 'atmospheric': 630,\n",
       " 'atom': 631,\n",
       " 'atomic': 632,\n",
       " 'attached': 633,\n",
       " 'attack': 634,\n",
       " 'attacked': 635,\n",
       " 'attacking': 636,\n",
       " 'attacks': 637,\n",
       " 'attained': 638,\n",
       " 'attempt': 639,\n",
       " 'attempted': 640,\n",
       " 'attempting': 641,\n",
       " 'attend': 642,\n",
       " 'attendance': 643,\n",
       " 'attended': 644,\n",
       " 'attendees': 645,\n",
       " 'attending': 646,\n",
       " 'attention': 647,\n",
       " 'attic': 648,\n",
       " 'attila': 649,\n",
       " 'attitude': 650,\n",
       " 'attraction': 651,\n",
       " 'attractive': 652,\n",
       " 'atv': 653,\n",
       " 'atåêcinema': 654,\n",
       " 'aubrey': 655,\n",
       " 'auburn': 656,\n",
       " 'auc': 657,\n",
       " 'auckland': 658,\n",
       " 'auctions': 659,\n",
       " 'audi': 660,\n",
       " 'audience': 661,\n",
       " 'audiences': 662,\n",
       " 'audio': 663,\n",
       " 'audit': 664,\n",
       " 'aug': 665,\n",
       " 'august': 666,\n",
       " 'aul': 667,\n",
       " 'aunt': 668,\n",
       " 'aurora': 669,\n",
       " 'aussie': 670,\n",
       " 'aussies': 671,\n",
       " 'aust': 672,\n",
       " 'austin': 673,\n",
       " 'australia': 674,\n",
       " 'australian': 675,\n",
       " 'australians': 676,\n",
       " 'austrian': 677,\n",
       " 'auth': 678,\n",
       " 'authentic': 679,\n",
       " 'authenticating': 680,\n",
       " 'author': 681,\n",
       " 'authorities': 682,\n",
       " 'authors': 683,\n",
       " 'autistic': 684,\n",
       " 'auto': 685,\n",
       " 'autobiography': 686,\n",
       " 'automatic': 687,\n",
       " 'automation': 688,\n",
       " 'autumn': 689,\n",
       " 'auz': 690,\n",
       " 'av': 691,\n",
       " 'ava': 692,\n",
       " 'available': 693,\n",
       " 'avalanche': 694,\n",
       " 'avalanches': 695,\n",
       " 'ave': 696,\n",
       " 'avenged': 697,\n",
       " 'avengers': 698,\n",
       " 'avenue': 699,\n",
       " 'average': 700,\n",
       " 'avert': 701,\n",
       " 'averted': 702,\n",
       " 'avi': 703,\n",
       " 'aviation': 704,\n",
       " 'avoid': 705,\n",
       " 'avoided': 706,\n",
       " 'avoiding': 707,\n",
       " 'avoids': 708,\n",
       " 'avysss': 709,\n",
       " 'aw': 710,\n",
       " 'await': 711,\n",
       " 'awaits': 712,\n",
       " 'awake': 713,\n",
       " 'awakenings': 714,\n",
       " 'awards': 715,\n",
       " 'aware': 716,\n",
       " 'awareness': 717,\n",
       " 'awash': 718,\n",
       " 'away': 719,\n",
       " 'awesome': 720,\n",
       " 'awesomeeeeeeee': 721,\n",
       " 'awesomelove': 722,\n",
       " 'awesomesauce': 723,\n",
       " 'awful': 724,\n",
       " 'awkward': 725,\n",
       " 'awn': 726,\n",
       " 'awol': 727,\n",
       " 'awwww': 728,\n",
       " 'ay': 729,\n",
       " 'ayhhhhhdjjfjrjjrdjjeks': 730,\n",
       " 'ayyo': 731,\n",
       " 'ayyy': 732,\n",
       " 'azed': 733,\n",
       " 'azin': 734,\n",
       " 'azing': 735,\n",
       " 'azingness': 736,\n",
       " 'azusa': 737,\n",
       " 'aåêmiddle': 738,\n",
       " 'ba': 739,\n",
       " 'baaaack': 740,\n",
       " 'baan': 741,\n",
       " 'babality': 742,\n",
       " 'babalmao': 743,\n",
       " 'babe': 744,\n",
       " 'babes': 745,\n",
       " 'babies': 746,\n",
       " 'baby': 747,\n",
       " 'bachmann': 748,\n",
       " 'backed': 749,\n",
       " 'background': 750,\n",
       " 'backing': 751,\n",
       " 'backlash': 752,\n",
       " 'backpack': 753,\n",
       " 'backroom': 754,\n",
       " 'backs': 755,\n",
       " 'backty': 756,\n",
       " 'backup': 757,\n",
       " 'backups': 758,\n",
       " 'backyard': 759,\n",
       " 'backyards': 760,\n",
       " 'bacteria': 761,\n",
       " 'bad': 762,\n",
       " 'badass': 763,\n",
       " 'badge': 764,\n",
       " 'badges': 765,\n",
       " 'badly': 766,\n",
       " 'badu': 767,\n",
       " 'bae': 768,\n",
       " 'baekhyun': 769,\n",
       " 'baffles': 770,\n",
       " 'baffling': 771,\n",
       " 'bag': 772,\n",
       " 'baggage': 773,\n",
       " 'bagged': 774,\n",
       " 'bagging': 775,\n",
       " 'bago': 776,\n",
       " 'bags': 777,\n",
       " 'bah': 778,\n",
       " 'bail': 779,\n",
       " 'bailed': 780,\n",
       " 'bairstow': 781,\n",
       " 'bait': 782,\n",
       " 'baiting': 783,\n",
       " 'bajrangi': 784,\n",
       " 'bak': 785,\n",
       " 'bake': 786,\n",
       " 'baked': 787,\n",
       " 'bal': 788,\n",
       " 'balance': 789,\n",
       " 'balanced': 790,\n",
       " 'balcony': 791,\n",
       " 'balding': 792,\n",
       " 'bali': 793,\n",
       " 'ball': 794,\n",
       " 'ballew': 795,\n",
       " 'balls': 796,\n",
       " 'baltimore': 797,\n",
       " 'ban': 798,\n",
       " 'banana': 799,\n",
       " 'band': 800,\n",
       " 'bandolier': 801,\n",
       " 'bands': 802,\n",
       " 'banerjee': 803,\n",
       " 'bang': 804,\n",
       " 'bangin': 805,\n",
       " 'bangladesh': 806,\n",
       " 'bangtan': 807,\n",
       " 'bank': 808,\n",
       " 'banki': 809,\n",
       " 'banks': 810,\n",
       " 'bankstown': 811,\n",
       " 'banned': 812,\n",
       " 'bannister': 813,\n",
       " 'banquet': 814,\n",
       " 'bans': 815,\n",
       " 'bar': 816,\n",
       " 'barack': 817,\n",
       " 'barak': 818,\n",
       " 'barbados': 819,\n",
       " 'barbaric': 820,\n",
       " 'barber': 821,\n",
       " 'barcelona': 822,\n",
       " 'barcousky': 823,\n",
       " 'bard': 824,\n",
       " 'bardo': 825,\n",
       " 'bare': 826,\n",
       " 'barely': 827,\n",
       " 'barfield': 828,\n",
       " 'bargain': 829,\n",
       " 'bark': 830,\n",
       " 'barkevious': 831,\n",
       " 'barking': 832,\n",
       " 'barn': 833,\n",
       " 'barra': 834,\n",
       " 'barracks': 835,\n",
       " 'barrier': 836,\n",
       " 'barring': 837,\n",
       " 'barrington': 838,\n",
       " 'barry': 839,\n",
       " 'bars': 840,\n",
       " 'bartender': 841,\n",
       " 'baruch': 842,\n",
       " 'basalt': 843,\n",
       " 'base': 844,\n",
       " 'baseball': 845,\n",
       " 'based': 846,\n",
       " 'baseman': 847,\n",
       " 'basement': 848,\n",
       " 'bash': 849,\n",
       " 'bashes': 850,\n",
       " 'bashing': 851,\n",
       " 'basic': 852,\n",
       " 'basically': 853,\n",
       " 'basics': 854,\n",
       " 'basis': 855,\n",
       " 'bask': 856,\n",
       " 'baskets': 857,\n",
       " 'bass': 858,\n",
       " 'bastard': 859,\n",
       " 'bastards': 860,\n",
       " 'bat': 861,\n",
       " 'bath': 862,\n",
       " 'bathe': 863,\n",
       " 'bathroom': 864,\n",
       " 'baths': 865,\n",
       " 'batista': 866,\n",
       " 'bats': 867,\n",
       " 'batter': 868,\n",
       " 'battered': 869,\n",
       " 'batteries': 870,\n",
       " 'batters': 871,\n",
       " 'battery': 872,\n",
       " 'batting': 873,\n",
       " 'battle': 874,\n",
       " 'battlefield': 875,\n",
       " 'battles': 876,\n",
       " 'battleship': 877,\n",
       " 'battleships': 878,\n",
       " 'battling': 879,\n",
       " 'bay': 880,\n",
       " 'bayelsa': 881,\n",
       " 'bayelsaåêstate': 882,\n",
       " 'bb': 883,\n",
       " 'bbc': 884,\n",
       " 'bbcnews': 885,\n",
       " 'bbm': 886,\n",
       " 'bc': 887,\n",
       " 'bck': 888,\n",
       " 'bcs': 889,\n",
       " 'bcuz': 890,\n",
       " 'bcz': 891,\n",
       " 'bd': 892,\n",
       " 'bday': 893,\n",
       " 'beach': 894,\n",
       " 'beached': 895,\n",
       " 'beacon': 896,\n",
       " 'beam': 897,\n",
       " 'bean': 898,\n",
       " 'bear': 899,\n",
       " 'beard': 900,\n",
       " 'bearer': 901,\n",
       " 'bearers': 902,\n",
       " 'bears': 903,\n",
       " 'beast': 904,\n",
       " 'beastin': 905,\n",
       " 'beastly': 906,\n",
       " 'beat': 907,\n",
       " 'beatdown': 908,\n",
       " 'beaten': 909,\n",
       " 'beating': 910,\n",
       " 'beats': 911,\n",
       " 'beatz': 912,\n",
       " 'beaumont': 913,\n",
       " 'beautiful': 914,\n",
       " 'beautifully': 915,\n",
       " 'beauty': 916,\n",
       " 'beaverton': 917,\n",
       " 'bebacksoon': 918,\n",
       " 'bece': 919,\n",
       " 'beck': 920,\n",
       " 'beckoning': 921,\n",
       " 'becyme': 922,\n",
       " 'bed': 923,\n",
       " 'bedding': 924,\n",
       " 'bedroom': 925,\n",
       " 'bedrooms': 926,\n",
       " 'bee': 927,\n",
       " 'beef': 928,\n",
       " 'beep': 929,\n",
       " 'beer': 930,\n",
       " 'bees': 931,\n",
       " 'beet': 932,\n",
       " 'beetroot': 933,\n",
       " 'beforeitsnews': 934,\n",
       " 'beg': 935,\n",
       " 'began': 936,\n",
       " 'begging': 937,\n",
       " 'begin': 938,\n",
       " 'beginners': 939,\n",
       " 'beginning': 940,\n",
       " 'beginnings': 941,\n",
       " 'begins': 942,\n",
       " 'begun': 943,\n",
       " 'behalf': 944,\n",
       " 'behavior': 945,\n",
       " 'behaviors': 946,\n",
       " 'behaviour': 947,\n",
       " 'behead': 948,\n",
       " 'beheadings': 949,\n",
       " 'behold': 950,\n",
       " 'beige': 951,\n",
       " 'beinart': 952,\n",
       " 'beisbol': 953,\n",
       " 'beit': 954,\n",
       " 'bel': 955,\n",
       " 'belie': 956,\n",
       " 'belief': 957,\n",
       " 'beliefs': 958,\n",
       " 'believe': 959,\n",
       " 'believing': 960,\n",
       " 'bell': 961,\n",
       " 'bella': 962,\n",
       " 'belle': 963,\n",
       " 'bellerin': 964,\n",
       " 'belligerent': 965,\n",
       " 'bells': 966,\n",
       " 'belly': 967,\n",
       " 'beloeil': 968,\n",
       " 'belonged': 969,\n",
       " 'belongs': 970,\n",
       " 'belt': 971,\n",
       " 'belter': 972,\n",
       " 'beluga': 973,\n",
       " 'ben': 974,\n",
       " 'benchmark': 975,\n",
       " 'bend': 976,\n",
       " 'benda': 977,\n",
       " 'bene': 978,\n",
       " 'beneath': 979,\n",
       " 'benedict': 980,\n",
       " 'benefits': 981,\n",
       " 'bengal': 982,\n",
       " 'bengalis': 983,\n",
       " 'bengston': 984,\n",
       " 'benitez': 985,\n",
       " 'benjin': 986,\n",
       " 'benothing': 987,\n",
       " 'benson': 988,\n",
       " 'benstracy': 989,\n",
       " 'bentley': 990,\n",
       " 'benzema': 991,\n",
       " 'berahino': 992,\n",
       " 'berlatsky': 993,\n",
       " 'berlin': 994,\n",
       " 'bernard': 995,\n",
       " 'bernardino': 996,\n",
       " 'berry': 997,\n",
       " 'besieged': 998,\n",
       " 'best': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 0, max_df = 5000, stop_words=ENGLISH_STOP_WORDS)\n",
    "\n",
    "X_train_counts = vectorizer.fit(df_train[\"text\"])\n",
    "word_freq = X_train_counts.vocabulary_\n",
    "\n",
    "word_freq = dict(sorted(word_freq.items(), key=lambda x: x[1], reverse=False))\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(word_freq.keys())\n",
    "\n",
    "with open(\"word.txt\",\"w+\") as f:\n",
    "    for word in word_list:\n",
    "        f.write(word +'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport spacy\\nnlp = spacy.load(\\'en_core_web_md\\')#small version\\n\\nline = nlp(u\\'{}\\'.format(\" \".join(word_list[5230:5240])))\\nfor w in line:\\n    print(f\"{w.text} {w.lemma_:>{20}}\")\\n    \\ndef lemmatization(text):\\n    raw_s = nlp(u\\'{}\\'.format(text))\\n    raw_s = \" \".join( [word.lemma_ for word in raw_s] )\\n    return(raw_s)\\n\\n#df_train[\"text\"] = df_train[\"text\"].apply(lemmatization)\\nline = \"I am going to destroy everything on my way!\"\\nprint(line)\\nlemmatization(line)\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')#small version\n",
    "\n",
    "line = nlp(u'{}'.format(\" \".join(word_list[5230:5240])))\n",
    "for w in line:\n",
    "    print(f\"{w.text} {w.lemma_:>{20}}\")\n",
    "    \n",
    "def lemmatization(text):\n",
    "    raw_s = nlp(u'{}'.format(text))\n",
    "    raw_s = \" \".join( [word.lemma_ for word in raw_s] )\n",
    "    return(raw_s)\n",
    "\n",
    "#df_train[\"text\"] = df_train[\"text\"].apply(lemmatization)\n",
    "line = \"I am going to destroy everything on my way!\"\n",
    "print(line)\n",
    "lemmatization(line)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word dictionary дал идею о:  \n",
    "- удалении ссылок\n",
    "- cоздании binary features: время (чуть улучшило), дата, город/страна (чуть ухудшило logistic regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_score(model): \n",
    "\n",
    "    # Form a prediction set\n",
    "    predictions = model.predict(X_test)\n",
    "    print(metrics.confusion_matrix(y_test,predictions))\n",
    "    \n",
    "    # Print a classification report\n",
    "    print(metrics.classification_report(y_test,predictions))\n",
    "    \n",
    "    # Print the overall accuracy\n",
    "    print(metrics.accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into train & test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[['text','time']]#,'GPE']]  # this time we want to look at the text\n",
    "\n",
    "#X = df_train['text']\n",
    "y = df_train['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['.','?','@','+',',','<','>','%','~','!','^','&','(',')',':',';']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2, max_df=0.6, stop_words=stopwords)\n",
    "vectorizer = vectorizer.fit(X_train.text)\n",
    "\n",
    "X_train_ = vectorizer.transform(X_train.text) # remember to use the original X_train set\n",
    "X_test_ = vectorizer.transform(X_test.text)\n",
    "\n",
    "#y_train = y_train.reshape(-1, 1)\n",
    "#y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "X_train_.shape# return sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Words which left in X_train_</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = vectorizer.vocabulary_\n",
    "word_freq = dict(sorted(word_freq.items(), key=lambda x: x[1], reverse=False))\n",
    "print(len(word_freq))\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Words which was recognized as *stop_words* and hence was deleted. This is result of params inside \"TfidfVectorizer\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vectorizer.stop_words_))\n",
    "print(vectorizer.stop_words_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X_train_ = hstack((X_train_, np.array([X_train.time.values]).T))\n",
    "X_test_ = hstack((X_test_, np.array([X_test.time.values]).T))\n",
    "\n",
    "X_train = hstack((X_train_, np.array([X_train.GPE.values]).T))\n",
    "X_test = hstack((X_test_, np.array([X_test.GPE.values]).T))\n",
    "'''\n",
    "\n",
    "X_train = hstack((X_train_, np.array([X_train.time.values]).T))\n",
    "X_test = hstack((X_test_, np.array([X_test.time.values]).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf_lr = LogisticRegression()\n",
    "#clf_lr.fit(X_train,y_train)\n",
    "#metric_score(clf_lr)\n",
    "\n",
    "\n",
    "model_list = [\n",
    "             KNeighborsClassifier(n_neighbors=10),\n",
    "             MultinomialNB(),\n",
    "             DecisionTreeClassifier(),\n",
    "    LGBMClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "             SVC(kernel=\"linear\"),\n",
    "             LogisticRegression()]\n",
    "\n",
    "\n",
    "for model in model_list[3:]:\n",
    "    print(f\"--------------\\n{type(model)}\")\n",
    "    model.fit(X_train,y_train)\n",
    "    metric_score(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to choose custom threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf_lr.predict(X_test)\n",
    "print(predictions[:5])\n",
    "#predict_proba()\n",
    "\n",
    "predictions = clf_lr.predict_proba(X_test)\n",
    "print(predictions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs = clf_lr.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# method I: plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(x_test, y_test):\n",
    "    threshold_list = list()\n",
    "    sensitivity_list = list()\n",
    "    specificity_list = list()\n",
    "    accuracy_list = list()\n",
    "\n",
    "    probs = clf_lr.predict_proba(x_test)\n",
    "    preds = probs[:,1]\n",
    "    \n",
    "    for threshold in np.arange(0.1,1,0.025):\n",
    "        threshold = round(threshold,2)\n",
    "\n",
    "        test_df = pd.DataFrame(y_test)\n",
    "        test_df[\"predicted\"] = preds\n",
    "\n",
    "        test_df[\"predicted\"] = test_df[\"predicted\"].apply(lambda x: 1 if x >threshold else 0)\n",
    "        CM = confusion_matrix(test_df.target, test_df.predicted)\n",
    "\n",
    "        TN = CM[0][0]\n",
    "        FN = CM[1][0]\n",
    "        TP = CM[1][1]\n",
    "        FP = CM[0][1]\n",
    "\n",
    "        sensitivity = TP/(TP+FN)\n",
    "        specificity = TN/(TN+FP)\n",
    "        accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
    "\n",
    "        threshold_list.append(threshold)\n",
    "        sensitivity_list.append(sensitivity)\n",
    "        specificity_list.append(specificity)\n",
    "        accuracy_list.append(accuracy)\n",
    "    return([threshold_list,sensitivity_list, specificity_list, accuracy_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(x_min, x_max, x_test,y_test):\n",
    "    threshold_list, sensitivity_list, specificity_list, accuracy_list = calculate_metrics(x_test, y_test)\n",
    "    x = threshold_list\n",
    "\n",
    "    fig=plt.figure(figsize=(6,6))\n",
    "    fig.show()\n",
    "    ax=fig.add_subplot(111)\n",
    "\n",
    "    ax.plot(x,sensitivity_list,c='r',label='Sensitivity',fillstyle='none')\n",
    "    ax.plot(x,specificity_list,c='blue',label='Specificity')\n",
    "    ax.plot(x,accuracy_list,c='black',label='Accuracy')\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=3)\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(0,1, X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(0.35,0.5, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train model on the whole training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronouns = \"\"\n",
    "with open(\"pronouns.txt\", \"r\") as f:\n",
    "    pronouns = f.read()\n",
    "#list(set(stopwords_pronouns))\n",
    "pronouns_list = list(set(pronouns.split(', ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[['text']]#,'GPE']]  # this time we want to look at the text\n",
    "\n",
    "#X = df_train['text']\n",
    "y = df_train['target']\n",
    "\n",
    "stopwords = ['.','?','@','+',',','<','>','%','~','!','^','&','(',')',':',';']\n",
    "\n",
    "stopwords_2 = ['a', 'about', 'an', 'and', 'are', 'as', 'at', 'be', 'been', 'but', 'by', 'can', \\\n",
    "             'even', 'ever', 'for', 'from', 'get', 'had', 'has', 'have', 'he', 'her', 'hers', 'his', \\\n",
    "             'how', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'just', 'me', 'my', 'of', 'on', 'or', \\\n",
    "             'see', 'seen', 'she', 'so', 'than', 'that', 'the', 'their', 'there', 'they', 'this', \\\n",
    "             'to', 'was', 'we', 'were', 'what', 'when', 'which', 'who', 'will', 'with', 'you']\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=2, max_df=0.6, stop_words=None)\n",
    "\n",
    "X_train = vectorizer.fit_transform(X.text) # remember to use the original X_train set\n",
    "#X_train = hstack((X_train_, np.array([X.time.values]).T))\n",
    "clf_lr = LogisticRegression()\n",
    "clf_lr.fit(X_train,y)\n",
    "\n",
    "predictions = clf_lr.predict(X_train)\n",
    "print(metrics.confusion_matrix(y,predictions))\n",
    "# Print a classification report\n",
    "print(metrics.classification_report(y,predictions))\n",
    "# Print the overall accuracy\n",
    "print(metrics.accuracy_score(y,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(0,1, X_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(.36, .6, X_train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and create csv for kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df = df_test[['text']]#,'GPE']]  # this time we want to look at the text\n",
    "\n",
    "X_test = vectorizer.transform(X_test_df.text) # remember to use the original X_train set\n",
    "#X_test = hstack((X_test_, np.array([X_test_df.time.values]).T))\n",
    "\n",
    "predictions = clf_lr.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"target\"] = predictions[:,1]\n",
    "df_test.drop([\"text\"], axis=1,inplace=True)\n",
    "df_test.drop([\"time\"], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_test.head(8))\n",
    "df_test.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"target\"] = df_test[\"target\"].apply(lambda x: 1 if x > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"nlp-getting-started/answer.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['a', 'about', 'an', 'and', 'are', 'as', 'at', 'be', 'been', 'but', 'by', 'can', \\\n",
    "             'even', 'ever', 'for', 'from', 'get', 'had', 'has', 'have', 'he', 'her', 'hers', 'his', \\\n",
    "             'how', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'just', 'me', 'my', 'of', 'on', 'or', \\\n",
    "             'see', 'seen', 'she', 'so', 'than', 'that', 'the', 'their', 'there', 'they', 'this', \\\n",
    "             'to', 'was', 'we', 'were', 'what', 'when', 'which', 'who', 'will', 'with', 'you']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer(stop_words=None)),\n",
    "                     ('clf', LinearSVC()),\n",
    "])\n",
    "\n",
    "\n",
    "# Naïve Bayes:\n",
    "text_clf_nb = Pipeline([('tfidf', TfidfVectorizer(stop_words=None)),\n",
    "                     ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf_logistic_regression = Pipeline([('tfidf', TfidfVectorizer(stop_words=None)),\n",
    "                                         ('clf', LogisticRegression()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the classifier and display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf.fit(X_train, y_train)  \n",
    "text_clf_nb.fit(X_train, y_train)  \n",
    "text_clf_logistic_regression.fit(X_train, y_train)  \n",
    "\n",
    "models_dict = {\"SVC\": text_clf, \"Naïve Bayes\":text_clf_nb, \n",
    "               \"Logistic_Reg.(threshold=0.5)\":text_clf_logistic_regression}\n",
    "for key, model in models_dict.items():\n",
    "    print(key)\n",
    "    metric_score(model)\n",
    "    print(\"-\"*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict values on test dataset & save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = text_clf.predict(df_test['text'])\n",
    "df_test[\"target\"] = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop([\"text\"], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"nlp-getting-started/answer.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чтобы попробывать  \n",
    "* EDA:\n",
    "    1. Dict of word frequency\n",
    "    2. Based on 1, calculate p-value for top-N the most/last frequent \n",
    "    \n",
    "\n",
    "\n",
    "* Logistic regression -> useing ROC to determine threshold value (not 0.5 maybe)  \n",
    "\n",
    "\n",
    "* советы с https://machinelearningmastery.com?\n",
    "\n",
    "### Вывод:  \n",
    "Из Dict of word frequency понял, что следует убрать все ссылки  \n",
    "Пример:  `http://t.co/zLvEbEoavG`.  \n",
    "*Ссылки не всегда в конце текста!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = 'small_bert/bert_en_uncased_L-12_H-768_A-12' \n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/2',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
