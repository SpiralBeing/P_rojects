{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Competition:** https://www.kaggle.com/c/nlp-getting-started/overview\n",
    "\n",
    "# Оглавление:  \n",
    "* [EDA](#1)\n",
    "* [BERT](#2)\n",
    "* [Выводы и итог](#3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "#https://www.kaggle.com/product-feedback/91185\n",
    "import bert_tokenization as tokenization\n",
    "#The file \"tokenization\" is forked from:\n",
    "#https://github.com/google-research/bert/blob/master/tokenization.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/nlp-getting-started/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5) (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "display(df_train.head())\n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop([\"keyword\",\"location\"], axis=1, inplace=True)\n",
    "df_test.drop([\"keyword\",\"location\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "text      0\n",
       "target    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train.target.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect & remove empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 blanks:  []\n"
     ]
    }
   ],
   "source": [
    "blanks = []\n",
    "\n",
    "for index, i, text, target in df_train.itertuples():  # iterate over the DataFrame\n",
    "    if type(text)==str:            # avoid NaN values\n",
    "        if text.isspace():         # test 'review' for whitespace\n",
    "            blanks.append(i) \n",
    "        \n",
    "print(len(blanks), 'blanks: ', blanks)\n",
    "\n",
    "df_train.drop(blanks, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. EDA <a id='1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Избавимся от ссылок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#stormchase Violent Record Breaking EF-5 El Reno Oklahoma Tornado Nearly Runs Over ... - http://t.co/3SICroAaNz http://t.co/I27Oa0HISp\n",
      "['http://t.co/3SICroAaNz', 'http://t.co/I27Oa0HISp']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#stormchase Violent Record Breaking EF-5 El Reno Oklahoma Tornado Nearly Runs Over ... -  '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = df_train[\"text\"].head(-5).values[-1]\n",
    "print(line)\n",
    "\n",
    "pattern = r'http://[/.\\w]+'#cuz maybe https://... or http://  or just http\n",
    "print(re.findall(pattern, line))\n",
    "\n",
    "re.sub(pattern,'',line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rid_of_link(text):\n",
    "    raw_s = r'{}'.format(text)\n",
    "    pattern = r'http[:/.\\w]+'\n",
    "    raw_s = re.sub(pattern,'',raw_s)\n",
    "    return(raw_s)\n",
    "\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(get_rid_of_link)\n",
    "df_test[\"text\"] = df_test[\"text\"].apply(get_rid_of_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Find time (am/pm/UTC/..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npattern = r\"[\\\\d]+:[\\\\d]+:[\\\\d]+\"\\npattern_2 = r\"[\\\\d]+:[\\\\d]+\"\\npattern_3 = r\"(am|pm|UTC)\"\\n\\npattern = r\"[\\\\d]+:[\\\\d]+:[\\\\d]+|[\\\\d]+:[\\\\d]+|am|pm|UTC\"\\n\\nline = r\"Earthquake : M 3.4 - 96km N of Brenas Puerto Rico: Time2015-08-05 10:34:24 UTC2015-08-05 06:34:24 -4:00 at\\x89Û_\"\\nprint(re.findall(pattern, line))\\nprint(re.sub(pattern, \"\",line))\\nprint(line)\\n\\nline = r\"Meow, Sparta\"\\nprint(re.findall(pattern, line))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pattern = r\"[\\d]+:[\\d]+:[\\d]+\"\n",
    "pattern_2 = r\"[\\d]+:[\\d]+\"\n",
    "pattern_3 = r\"(am|pm|UTC)\"\n",
    "\n",
    "pattern = r\"[\\d]+:[\\d]+:[\\d]+|[\\d]+:[\\d]+|am|pm|UTC\"\n",
    "\n",
    "line = r\"Earthquake : M 3.4 - 96km N of Brenas Puerto Rico: Time2015-08-05 10:34:24 UTC2015-08-05 06:34:24 -4:00 atÛ_\"\n",
    "print(re.findall(pattern, line))\n",
    "print(re.sub(pattern, \"\",line))\n",
    "print(line)\n",
    "\n",
    "line = r\"Meow, Sparta\"\n",
    "print(re.findall(pattern, line))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef create_bool_time_feature(text):\\n    raw_s = r\\'{}\\'.format(text)\\n    pattern = r\"[\\\\d]+:[\\\\d]+:[\\\\d]+|[\\\\d]+:[\\\\d]+|am|pm|UTC\"\\n    if(len(re.findall(pattern, raw_s))!=0):\\n        return(1)\\n    else:\\n        return(0)\\n    \\ndef get_rid_of_time(text):\\n    raw_s = r\\'{}\\'.format(text)\\n    pattern = r\"[\\\\d]+:[\\\\d]+:[\\\\d]+|[\\\\d]+:[\\\\d]+|am|pm|UTC\"\\n    raw_s = re.sub(pattern,\\'\\',raw_s)\\n    return(raw_s)\\n\\ndf_train[\"time\"] = df_train[\"text\"].apply(create_bool_time_feature)\\ndf_test[\"time\"] = df_test[\"text\"].apply(create_bool_time_feature)\\n\\ndf_train[\"text\"] = df_train[\"text\"].apply(get_rid_of_time)\\ndf_test[\"text\"] = df_test[\"text\"].apply(get_rid_of_time)\\n\\ndf_train[\"time\"].value_counts()\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def create_bool_time_feature(text):\n",
    "    raw_s = r'{}'.format(text)\n",
    "    pattern = r\"[\\d]+:[\\d]+:[\\d]+|[\\d]+:[\\d]+|am|pm|UTC\"\n",
    "    if(len(re.findall(pattern, raw_s))!=0):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "    \n",
    "def get_rid_of_time(text):\n",
    "    raw_s = r'{}'.format(text)\n",
    "    pattern = r\"[\\d]+:[\\d]+:[\\d]+|[\\d]+:[\\d]+|am|pm|UTC\"\n",
    "    raw_s = re.sub(pattern,'',raw_s)\n",
    "    return(raw_s)\n",
    "\n",
    "df_train[\"time\"] = df_train[\"text\"].apply(create_bool_time_feature)\n",
    "df_test[\"time\"] = df_test[\"text\"].apply(create_bool_time_feature)\n",
    "\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(get_rid_of_time)\n",
    "df_test[\"text\"] = df_test[\"text\"].apply(get_rid_of_time)\n",
    "\n",
    "df_train[\"time\"].value_counts()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Удалим дубликаты \n",
    "Есть дубликаты. Некоторые наблюдения полностью совпадают по \"text\", некоторые отличаются орфографической ошибкой в тексте.  \n",
    "Удалим те, что полностью идентичны по feature \"text\" (значения \"taget\" порой разные)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of observations: (7613,),\n",
      "Number of unique observations: (6989,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Amount of observations: {df_train.text.shape},\\nNumber of unique observations: {df_train.text.unique().shape}\")\n",
    "df_train = df_train.drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.  Удаление всех токенов вида цифры/цифры+слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rid_of_digits(text):\n",
    "    raw_s = r'{}'.format(text)\n",
    "    pattern = r\"\\d+\\w+|\\w+\\d+\"\n",
    "    raw_s = re.sub(pattern,'',raw_s)\n",
    "    return(raw_s)\n",
    "\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(get_rid_of_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Удаление Тэгов (`#`... и @....) и слов с подчеркиванием (_ashj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rid_of_tags(text):\n",
    "    raw_s = r'{}'.format(text)\n",
    "    pattern = r\"@\\w+|#\\w+|_+\\w+|\\w+_|\"\n",
    "    raw_s = re.sub(pattern,'',raw_s)\n",
    "    return(raw_s)\n",
    "\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(get_rid_of_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Отбор слов (создание списка stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 0, max_df = 5000, stop_words=ENGLISH_STOP_WORDS)\n",
    "\n",
    "X_train_counts = vectorizer.fit(df_train[\"text\"])\n",
    "word_freq = X_train_counts.vocabulary_\n",
    "\n",
    "word_freq = dict(sorted(word_freq.items(), key=lambda x: x[1], reverse=False))\n",
    "word_freq\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 2. BERT  <a id='2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    print(sequence_output)\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    print(clf_output.shape)\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(SGD(lr=0.0001, momentum=0.8), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.2 s, sys: 3.09 s, total: 15.3 s\n",
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)# Choice of the BERT model is very important! There are some model with 300m+ params! (I got bad result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (6989, 128)\n"
     ]
    }
   ],
   "source": [
    "train = df_train\n",
    "max_len = 128\n",
    "\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "train_input = bert_encode(train.text.values, tokenizer, max_len=max_len)\n",
    "train_labels = train.target.values\n",
    "\n",
    "print(len(train_input), train_input[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"keras_layer/cond/Identity_1:0\", shape=(None, None, 768), dtype=float32)\n",
      "(None, 768)\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 768)]        0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            769         tf_op_layer_strided_slice[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 109,483,010\n",
      "Trainable params: 109,483,009\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_layer, max_len=max_len)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "175/175 [==============================] - 86s 494ms/step - loss: 0.6307 - accuracy: 0.6493 - val_loss: 0.5681 - val_accuracy: 0.7210\n",
      "Epoch 2/10\n",
      "175/175 [==============================] - 86s 490ms/step - loss: 0.5214 - accuracy: 0.7501 - val_loss: 0.4890 - val_accuracy: 0.7747\n",
      "Epoch 3/10\n",
      "175/175 [==============================] - 86s 490ms/step - loss: 0.4735 - accuracy: 0.7850 - val_loss: 0.4619 - val_accuracy: 0.7868\n",
      "Epoch 4/10\n",
      "175/175 [==============================] - 86s 490ms/step - loss: 0.4466 - accuracy: 0.8036 - val_loss: 0.4433 - val_accuracy: 0.8047\n",
      "Epoch 5/10\n",
      "175/175 [==============================] - 85s 488ms/step - loss: 0.4287 - accuracy: 0.8135 - val_loss: 0.4309 - val_accuracy: 0.8062\n",
      "Epoch 6/10\n",
      "175/175 [==============================] - 85s 489ms/step - loss: 0.4149 - accuracy: 0.8226 - val_loss: 0.4241 - val_accuracy: 0.8097\n",
      "Epoch 7/10\n",
      "175/175 [==============================] - 86s 490ms/step - loss: 0.4028 - accuracy: 0.8287 - val_loss: 0.4200 - val_accuracy: 0.8162\n",
      "Epoch 8/10\n",
      "175/175 [==============================] - 85s 489ms/step - loss: 0.3914 - accuracy: 0.8342 - val_loss: 0.4227 - val_accuracy: 0.8219\n",
      "Epoch 9/10\n",
      "175/175 [==============================] - 83s 475ms/step - loss: 0.3835 - accuracy: 0.8378 - val_loss: 0.4118 - val_accuracy: 0.8205\n",
      "Epoch 10/10\n",
      "175/175 [==============================] - 83s 475ms/step - loss: 0.3739 - accuracy: 0.8426 - val_loss: 0.4100 - val_accuracy: 0.8212\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "train_history = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    callbacks=[checkpoint],\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict test dataset to submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = bert_encode(df_test.text.values, tokenizer, max_len=max_len)\n",
    "test_pred = model.predict(test_input)\n",
    "submission = train.truncate(after = -1)\n",
    "submission['id'] = df_test['id']\n",
    "submission['text'] = df_test['text']\n",
    "submission['target'] = test_pred.round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission[['id','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"./answer.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "175/175 [==============================] - 85s 487ms/step - loss: 0.3647 - accuracy: 0.8467 - val_loss: 0.4092 - val_accuracy: 0.8233\n",
      "Epoch 2/10\n",
      "175/175 [==============================] - 83s 474ms/step - loss: 0.3560 - accuracy: 0.8517 - val_loss: 0.4076 - val_accuracy: 0.8205\n",
      "Epoch 3/10\n",
      "175/175 [==============================] - 83s 474ms/step - loss: 0.3479 - accuracy: 0.8567 - val_loss: 0.4080 - val_accuracy: 0.8212\n",
      "Epoch 4/10\n",
      "175/175 [==============================] - 83s 474ms/step - loss: 0.3388 - accuracy: 0.8634 - val_loss: 0.4092 - val_accuracy: 0.8197\n",
      "Epoch 5/10\n",
      "175/175 [==============================] - 83s 474ms/step - loss: 0.3308 - accuracy: 0.8655 - val_loss: 0.4109 - val_accuracy: 0.8190\n",
      "Epoch 6/10\n",
      "175/175 [==============================] - 83s 474ms/step - loss: 0.3214 - accuracy: 0.8741 - val_loss: 0.4102 - val_accuracy: 0.8233\n",
      "Epoch 7/10\n",
      "175/175 [==============================] - 83s 475ms/step - loss: 0.3120 - accuracy: 0.8771 - val_loss: 0.4148 - val_accuracy: 0.8226\n",
      "Epoch 8/10\n",
      "175/175 [==============================] - 85s 488ms/step - loss: 0.3034 - accuracy: 0.8814 - val_loss: 0.4154 - val_accuracy: 0.8240\n",
      "Epoch 9/10\n",
      "175/175 [==============================] - 83s 474ms/step - loss: 0.2944 - accuracy: 0.8864 - val_loss: 0.4150 - val_accuracy: 0.8219\n",
      "Epoch 10/10\n",
      "175/175 [==============================] - 83s 474ms/step - loss: 0.2843 - accuracy: 0.8914 - val_loss: 0.4232 - val_accuracy: 0.8226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff378fd5c50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load_model = tf.keras.models.load_model('my_model.h5')\n",
    "\n",
    "# retraining the model\n",
    "model.fit(train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    callbacks=[checkpoint],\n",
    "    batch_size=32,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы и score в kaggle: <a id='3'></a>    \n",
    "- Метрику f1-score и confusion matrix не использовал, поскольку имеется лишь небольшой дисбаланс между классами у целевой переменной.      \n",
    "- При тестировании классических и бустинг алгоритмов ML использовалась дополнительная предобработка. Однако это позволило лишь дойти до топ-45% leaderboard. \n",
    "- Использовались вычислительные мощности kaggle'а - NVidia K80 GPUs.      \n",
    "- С таким notebook попал в топ-17% (200/1245) leaderboard.  \n",
    "https://www.kaggle.com/c/nlp-getting-started/leaderboard  \n",
    "https://www.kaggle.com/konstantinlp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
